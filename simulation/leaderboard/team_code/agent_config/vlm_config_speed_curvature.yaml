control:
    turn_KP: 1.0
    turn_KI: 0.2
    turn_KD: 0.1
    turn_n: 5  # buffer size

    speed_KP: 5.0
    speed_KI: 1
    speed_KD: 0.1
    speed_n: 20  # buffer size

    max_throttle: 0.75             # upper limit on throttle signal value in dataset
    brake_speed: 0.1               # desired speed below which brake is triggered
    brake_ratio: 1.1               # ratio of speed to desired speed at which brake is triggered
    clip_delta: 0.35               # maximum change in speed input to logitudinal controller

    max_speed: 5                   # the max speed of ego vehicle
    collision_buffer: [2.5, 1.2]   # safe distance in vertical and horizontal
    momentum: 0                    #
    target_point_distance: 10      # distance from ego vehicle to the target point generated by global navigation planner
    detect_threshold: &det_thre  0.04

# rsu config
RSU:
    change_rsu_frame: 5            # frame gap before a new rsu is spawned
    rsu_height: 7.5                # height of rsu
    rsu_lane_side: 'right'         # location of rsu, on the right/left side of the lane
    rsu_distance: 12               # horizontal distance from rsu to ego vehicle, positive value means foward direction

simulation:
    ego_num: 2                     # number of communicating drivable ego vehicles
    skip_frames: 4                 # frame gap before a new driving control signal is generated

heter:
    avail_heter_planner_configs: ["vlmdrive/hypes_yaml/api_vlm_drive_speed_curvature_qwen2.5-72b-awq.yaml", "vlmdrive/hypes_yaml/api_vlm_drive_speed_curvature_qwen2.5-3b-awq.yaml", "vlmdrive/hypes_yaml/api_vlm_drive_speed_curvature_qwen2.5-7b-awq.yaml"]
    ego_planner_choice: [1, 2]

perception:
    core_method: 'collaborative_perception'
    detect_threshold: *det_thre        #
    detection_range: [36, 12, 12, 12] # front rear left right
    # perception model directory, including model checkpoint and config file
    perception_model_dir: 'opencood/logs/v2xverse_late_multiclass_2025_01_28_08_49_56'

planning: 
    core_method: 'coopenemma'
    planner_config: "vlmdrive/hypes_yaml/api_vlm_drive_speed_curvature.yaml"

    # prompt config
    prompt_template:
        scene_prompt_template: 
            default: "You are a autonomous driving vehicle controller. You have access to a front-view camera image.  Describe broadly describes the driving scenarios, including weather, traffic situations, and road conditions."
            llava: "You are a autonomous driving vehicle controller. You have access to a front-view camera images of a vehicle. Based on the lane markings and the movement of other cars and pedestrians, provide a concise description of the desired intent of  the ego car. Is it going to follow the lane to turn left, turn right, or go straight? Should it maintain the current speed or slow down or speed up?"
        object_prompt_template: 
            default: "You are a autonomous driving vehicle controller. You have access to a front-view camera image. What other road users should you pay attention to in the driving scene? List two or three of them, specifying its location within the image of the driving scene and provide a short description of the that road user's current status and intent."
        intention_prompt_template: "currently not used"
        sys_message: "currently not used"
        comb_prompt: # note the gen_prompts currently needs to modify the code in base_vlm_planner.py
            default: |
                Here is the description of the environment of a car and detected objects around it.
                The scene is described as follows: {scene_description}.  
                The identified critical objects and corresponding intents are {object_description}.  
                The historical information of the ego vehicle is as follows: {ego_history_prompt}.  
                You need to generate vehicle's future speed and curvature for the next 10 steps and ensure these waypoints allow the vehicle to move forward to the target waypoint {target_waypoint}.
                Output MUST be a valid JSON structure with the key \"predicted_speeds_curvatures\" containing a list of 10 [speed, curvature] pairs.
                Make sure it strictly follows this JSON format:
                {{
                    "predicted_speeds_curvatures": [
                        [speed_1, curvature_1],
                        [speed_2, curvature_2],
                        ...,
                        [speed_10, curvature_10]
                    ]
                }}
                No additional keys or text outside of this JSON are allowed
                ```

