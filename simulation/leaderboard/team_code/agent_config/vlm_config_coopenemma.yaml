control:
    turn_KP: 1.0
    turn_KI: 0.2
    turn_KD: 0.1
    turn_n: 5  # buffer size

    speed_KP: 5.0
    speed_KI: 1
    speed_KD: 0.1
    speed_n: 20  # buffer size

    max_throttle: 0.75             # upper limit on throttle signal value in dataset
    brake_speed: 0.1               # desired speed below which brake is triggered
    brake_ratio: 1.1               # ratio of speed to desired speed at which brake is triggered
    clip_delta: 0.35               # maximum change in speed input to logitudinal controller

    max_speed: 5                   # the max speed of ego vehicle
    collision_buffer: [2.5, 1.2]   # safe distance in vertical and horizontal
    momentum: 0                    #
    target_point_distance: 10      # distance from ego vehicle to the target point generated by global navigation planner
    detect_threshold: &det_thre  0.04

# rsu config
RSU:
    change_rsu_frame: 5            # frame gap before a new rsu is spawned
    rsu_height: 7.5                # height of rsu
    rsu_lane_side: 'right'         # location of rsu, on the right/left side of the lane
    rsu_distance: 12               # horizontal distance from rsu to ego vehicle, positive value means foward direction

simulation:
    ego_num: 1                     # number of communicating drivable ego vehicles
    skip_frames: 4                 # frame gap before a new driving control signal is generated

perception:
    core_method: 'collaborative_perception'
    detect_threshold: *det_thre        #
    detection_range: [36, 12, 12, 12] # front rear left right
    # perception model directory, including model checkpoint and config file
    perception_model_dir: 'opencood/logs/v2xverse_late_multiclass_2025_01_28_08_49_56'

planning: 
    core_method: 'coopenemma'
    planner_config: "vlmdrive/hypes_yaml/llava_drive.yaml"

    # prompt config
    prompt_template:
        scene_prompt_template: 
            default: "You are a autonomous driving labeller. You have access to a front-view camera images of a car. Imagine you are driving the car. Describe the driving scene according to traffic lights, other cars or pedestrians and lane markings."
            llava: "You are a autonomous driving labeller. You have access to a front-view camera images of a vehicle. Imagine you are driving the car. Based on the lane markings and the movement of other cars and pedestrians, provide a concise description of the desired intent of  the ego car. Is it going to follow the lane to turn left, turn right, or go straight? Should it maintain the current speed or slow down or speed up?"
        object_prompt_template: 
            default: "You are a autonomous driving labeller. You have access to a front-view camera images of a vehicle. Imagine you are driving the car. What other road users should you pay attention to in the driving scene? List two or three of them, specifying its location within the image of the driving scene and provide a short description of the that road user on what it is doing, and why it is important to you."
        intention_prompt_template: "currently not used"
        sys_message: "currently not used"
        comb_prompt: # note the gen_prompts currently needs to modify the code in base_vlm_planner.py
            default: |
                Here is the description of the environment of a car and detected objects around it.
                The scene is described as follows: {scene_description}.
                The identified critical objects are {object_description}.
                The summarized table and my goal and target are as follows: {gen_prompts}.
